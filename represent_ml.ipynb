{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas version is  0.20.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import timeit\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from natsort import natsorted\n",
    "import itertools\n",
    "from collections import Counter, OrderedDict\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from scipy.spatial.distance import cosine, sqeuclidean\n",
    "from sklearn import decomposition, preprocessing, cluster, manifold\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score, ShuffleSplit, RandomizedSearchCV,\\\n",
    "GridSearchCV\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from scipy.spatial.distance import squareform, pdist\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_kernels\n",
    "from IPython.display import display, HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff_perc_with_conservative\n",
      "diff_perc_with_labour\n",
      "diff_perc_with_snp\n",
      "diff_perc_with_liberaldemocrat\n",
      "diff_perc_with_plaidcymru\n",
      "diff_perc_with_green\n",
      "diff_perc_with_ukip\n"
     ]
    }
   ],
   "source": [
    "#LOADING DATA\n",
    "\n",
    "\n",
    "question_table = pd.read_csv('data/app_question',delimiter='|')\n",
    "questionchoice_table = pd.read_csv('data/app_questionchoice',delimiter=',')\n",
    "questioncollection_table = pd.read_csv('data/app_questioncollection',delimiter='|')\n",
    "questioncollectionitem_table = pd.read_csv('data/app_questioncollectionitem',delimiter='|')\n",
    "questiontag_table = pd.read_csv('data/app_questiontag',delimiter='|')\n",
    "questiontagvote_table = pd.read_csv('data/app_questiontagvote',delimiter='|')\n",
    "questionvote_table = pd.read_csv('data/app_questionvote.csv',delimiter='|',dtype='object',parse_dates=['modified_at',\\\n",
    "                                                                                                       'created_at'])\n",
    "tag_table = pd.read_csv('data/app_tag',delimiter=';',dtype='object')\n",
    "usercompare_table = pd.read_csv('data/app_usercompare',delimiter='|',dtype='object')\n",
    "userextra_table = pd.read_csv('data/app_userextra',delimiter='|')\n",
    "usergeo_table = pd.read_csv('data/app_usergeo',delimiter='|')\n",
    "geo_table = pd.read_csv('data/app_geo',delimiter='|')\n",
    "\n",
    "all_users_compared_table = pd.read_csv('data/all_users_compared.csv',delimiter=',',dtype='object')\n",
    "users_info_table = pd.read_csv('data/users_info.csv',delimiter=',')\n",
    "\n",
    "#CLEANING DATA\n",
    "\n",
    "userextra_table.drop(['password','first_name','last_name','photo','email'],axis=1,inplace=True) #delete personal data\n",
    "users_info_table.drop(['count_tags'],axis=1,inplace=True)\n",
    "\n",
    "#select last active votes\n",
    "questionvote_table = questionvote_table[questionvote_table['active']=='t']\n",
    "\n",
    "#convert to integers \n",
    "questionvote_table['object_id'] = questionvote_table['object_id'].astype(int)\n",
    "questionvote_table['user_id'] = questionvote_table['user_id'].astype(int)\n",
    "tag_table['id'] = tag_table['id'].astype(int)\n",
    "question_table['id'] = question_table['id'].astype(int)\n",
    "#delete duplicate votes\n",
    "questionvote_table.drop_duplicates(subset=['user_id','object_id'],inplace=True)\n",
    "\n",
    "#some geo data is faulty in some places, here's a hardcode fix\n",
    "geo_fix = {'Medway':'England', 'Moray':'Scotland', 'Argyll and Bute':'Scotland', 'Ards':'N. Ireland'}\n",
    "users_info_table.region = users_info_table.region.replace(geo_fix)\n",
    "\n",
    "#make gender data more readable\n",
    "genders={1.0:'male',2.0:'female',3.0:'other'}\n",
    "users_info_table.gender.replace(genders, inplace=True)\n",
    "#users_info_table.gender.fillna(value='none',inplace=True)\n",
    "\n",
    "#replace 0 with NaN so that numpy ignores it in calculations\n",
    "\n",
    "users_info_table.replace(0,np.NaN,inplace=True)\n",
    "\n",
    "#convert difference to similarity\n",
    "\n",
    "party_variables = ['diff_perc_with_labour','diff_perc_with_conservative','diff_perc_with_green', \\\n",
    "            'diff_perc_with_plaidcymru', 'diff_perc_with_ukip', 'diff_perc_with_snp',\\\n",
    "                   'diff_perc_with_liberaldemocrat']\n",
    "for party_variable in party_variables:\n",
    "    users_info_table[party_variable] = 100 - users_info_table[party_variable]\n",
    "\n",
    "\n",
    "#merge tags with questions\n",
    "\n",
    "questions_with_tags = pd.merge(questiontag_table, tag_table[['id','text']], left_on='tag_id',right_on='id')\n",
    "questions_with_tags = questions_with_tags.groupby('question_id')['text'].apply(lambda x:', '.join(x.str.lower())).reset_index()\n",
    "questions_with_tags = pd.DataFrame(questions_with_tags)\n",
    "questions_with_tags.rename(columns={'text':'tags'},inplace=True)\n",
    "questions_with_tags = pd.merge(question_table, questions_with_tags,left_on='id',right_on='question_id',how='left')\n",
    "#questions_with_tags.drop('question_id',axis=1,inplace=True)\n",
    "\n",
    "\n",
    "all_users_compared_table.difference_percent = all_users_compared_table.difference_percent.astype(float)\n",
    "all_users_compared_table[['usera_id','userb_id']] = all_users_compared_table[['usera_id','userb_id']].astype(int)\n",
    "all_users_compared_table.difference_percent = 100 - all_users_compared_table.difference_percent\n",
    "\n",
    "parties = {'labour': 17351 , 'conservative': 17663, 'green': 17687 ,'plaidcymru': 17689, 'ukip': 17710, \n",
    "           'snp': 17711, 'liberaldemocrat': 17692}\n",
    "\n",
    "for party in parties:\n",
    "    column_name = 'diff_perc_with_' + party\n",
    "    print(column_name)\n",
    "    dummy_df = all_users_compared_table[all_users_compared_table.userb_id==parties[party]][['usera_id','difference_percent']]\n",
    "\n",
    "   \n",
    "    merged_df = pd.merge(users_info_table,dummy_df,left_on='id',right_on='usera_id')['difference_percent']\n",
    "    users_info_table.loc[users_info_table[column_name].isnull(), column_name] = merged_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "british_users = users_info_table[users_info_table['country']=='United Kingdom']['id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_vote_data(question_ids,user_ids,rescale_data=False,agg_function=float,question_threshold=500,\\\n",
    "                     user_threshold=20,fillna_value=0,sort_index=True,cell_type=float):\n",
    "    \n",
    "    if user_ids!=None:\n",
    "        filtered = questionvote_table[questionvote_table['user_id'].isin(user_ids)]\n",
    "    else:\n",
    "        filtered = questionvote_table\n",
    "        \n",
    "    data = filtered.pivot_table(index='user_id',columns='object_id',values='value',aggfunc=lambda x: agg_function(x))\n",
    "    \n",
    "    data.dropna(thresh = question_threshold, axis=1,inplace=True) #'more than 'question_threshold votes for a question'\n",
    "    data.dropna(thresh = user_threshold, axis=0,inplace=True) #'more than 'user_threshold' votes from a user'\n",
    "    data.fillna(value = fillna_value, inplace=True)\n",
    "    \n",
    "    data.index.names = ['user_id']\n",
    "    \n",
    "    if sort_index==True:\n",
    "        data.sort_index(inplace=True)\n",
    "        \n",
    "    data = data.astype(cell_type)\n",
    "    if rescale_data==True:\n",
    "        replace_dict = {1.0:-2.0, 2.0:-1.0, 3.0:0.0, 4.0:1.0, 5.0:2.0}\n",
    "        data.replace(replace_dict, inplace=True)\n",
    "    print(data.shape[0],'users')\n",
    "    print(data.shape[1],'questions')\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#SCALING DATA\n",
    "def scale_data(data, scale_type = preprocessing.StandardScaler()):\n",
    "    \n",
    "    if scale_type!=None:\n",
    "        print('the scaling function is ', scale_type)\n",
    "        return pd.DataFrame(scale_type.fit_transform(data), columns=data.columns,\\\n",
    "                            index=data.index)\n",
    "    \n",
    "    else:\n",
    "        return data\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_example_plot(labels, centers, colmap):\n",
    "    #EXAMPLE 3D PLOT. 3 COMPONENTS RECOMMENDED\n",
    "    \n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "    ax = Axes3D(fig)\n",
    "    \n",
    "    colors = list(map(lambda x: colmap[x+1], labels))\n",
    "    ax.scatter( questions_components['Component 1'], questions_components['Component 2'], questions_components['Component 3'],color=colors, alpha=0.5, edgecolor='k')\n",
    "\n",
    "\n",
    "    # for idx, centroid in enumerate(centroids):\n",
    "    #     plt.scatter(*centroid, color=colmap[idx+1])\n",
    "\n",
    "    plt.xlim(-0.1, 0.1)\n",
    "    plt.ylim(-0.1, 0.1)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "def PCA_items_components(data, n_components,use_projection=False,transpose=False):\n",
    "    \n",
    "    pca = decomposition.PCA(n_components = n_components)\n",
    "    projection = pca.fit_transform(data)\n",
    "    \n",
    "    print('Explained variance is ',sum(pca.explained_variance_ratio_))\n",
    "    if use_projection==False:\n",
    "        components_names = ['Component '+str(i) for i in range(1,n_components+1)]\n",
    "        items_components = pd.DataFrame(pca.components_,columns = data.columns, \\\n",
    "                                            index = components_names)\n",
    "        if transpose==True:\n",
    "            items_components = items_components.T\n",
    "\n",
    "    else:\n",
    "        items_components = pd.DataFrame(projection)\n",
    "\n",
    "    return items_components\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FA\n",
    "\n",
    "def FA_total_var(components,noise_variance):\n",
    "\n",
    "    comp = components\n",
    "    sumcol = np.sum(comp**2,axis=1)\n",
    "\n",
    "    print('Explained variance',sum([(100*sumcol[i])/(np.sum(sumcol)+np.sum(noise_variance))\n",
    "                                    for i in range(components.shape[0])]))\n",
    "    \n",
    "    \n",
    "def FA_items_components(data, n_components,use_projection=False,transpose=False):\n",
    "    \n",
    "    fa = decomposition.FactorAnalysis(n_components = n_components)\n",
    "    projection = fa.fit_transform(data)\n",
    "    FA_total_var(fa.components_,fa.noise_variance_)\n",
    "    if use_projection==False:\n",
    "        components_names = ['Component '+str(i) for i in range(1,n_components+1)]\n",
    "        items_components = pd.DataFrame(fa.components_,columns = data.columns, \\\n",
    "                                            index = components_names)\n",
    "        if transpose==True:\n",
    "            items_components = items_components.T\n",
    "\n",
    "    else:\n",
    "        items_components = pd.DataFrame(projection)\n",
    "\n",
    "    return items_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#MDS\n",
    "def MDS_items_components(data, n_components, metric, use_projection=False):\n",
    "    \n",
    "    mds = manifold.MDS(n_components = n_components,metric = metric, max_iter=100, verbose=1, n_jobs=1)\n",
    "    projection = mds.fit_transform(data)\n",
    "    if use_projection==False:\n",
    "        print('NOT SUPPORTED')\n",
    "        return None\n",
    "    else:\n",
    "        items_components = pd.DataFrame(projection)\n",
    "        print('stress value',mds.stress_)\n",
    "        return items_components\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_cluster_items_dict(data):\n",
    "\n",
    "    # cluster : [list of item ids]\n",
    "    clusters_dict = { cluster : [str(x) for x in data.T[data.T['label']==cluster].index.unique().tolist()] \\\n",
    "                 for cluster in data.T['label']}\n",
    "\n",
    "    return clusters_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_item_cluster_dict(data):\n",
    "        #{item : cluster for every item in data}\n",
    "        \n",
    "    cl = {}\n",
    "    for item in data.columns:\n",
    "        cl[item] = data[item]['label']\n",
    "    return cl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_analysis_cluster(questions, relevant_columns, threshold=10, verbose=False):\n",
    "    \n",
    "    questions_df = questions_with_tags[questions_with_tags.id.isin(questions)]\n",
    "    \n",
    "    questions_df = questions_df[[column for column in relevant_columns+['tags']]]\n",
    "    \n",
    "    #list of tags for every question\n",
    "    all_tags = questions_df[questions_df.tags.notnull()].tags.str.cat(sep=' ').split(', ')\n",
    "\n",
    "    #merge all tags for all questions\n",
    "    all_tags = ' '.join(s for s in all_tags).split(' ')\n",
    "\n",
    "    #count the tags\n",
    "    cnt = Counter(all_tags)\n",
    "    most_common_tags = [i[0] for i in list(cnt.most_common(threshold))]\n",
    "\n",
    "    results = OrderedDict()\n",
    "    \n",
    "    results_dict = {'avg_liquid_vote_count' : questions_df.liquid_vote_count.mean(),\n",
    "               'avg_polarisation': questions_df.polarisation.mean(),\n",
    "               'avg_count_comments': questions_df.count_comments.mean(),\n",
    "               'avg_liquid_consensus': questions_df.liquid_consensus.mean()}\n",
    "    \n",
    "    tags = {'tag '+str(i): most_common_tags[i-1] if i<len(most_common_tags)+1 else 0 for i in range(1,threshold+1) }\n",
    "\n",
    "    results.update(results_dict)\n",
    "    \n",
    "    new_tags = OrderedDict(natsorted(tags.items()))\n",
    "\n",
    "    results.update(new_tags)\n",
    "\n",
    "    return results\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "analysis_question_variables = ['id','liquid_vote_count','polarisation','count_comments','liquid_consensus'] \n",
    "\n",
    "\n",
    "analysis_user_variables = ['age','gender',\n",
    "        'region', 'count_comment_votes', 'count_friends',\n",
    "       'count_group_memberships', 'count_question_votes', 'count_questions',\n",
    "       'count_votes', 'count_choice_votes','count_following_tags', 'count_comments',\\\n",
    "        'count_followers','count_following_users', 'verification_score', 'verification_count',\n",
    "       'diff_perc_with_labour', 'diff_perc_with_conservative',\n",
    "       'diff_perc_with_green', 'diff_perc_with_plaidcymru',\n",
    "       'diff_perc_with_ukip', 'diff_perc_with_snp',\n",
    "       'diff_perc_with_liberaldemocrat', 'session_count',\n",
    "       'count_votes_last_3_months', 'count_choice_votes_last_3_months',\n",
    "       'count_compares_last_3_months', 'count_comments_last_3_months',\n",
    "       'count_group_membership_last_3_months', 'count_votes_last_month',\n",
    "       'count_choice_votes_last_month', 'count_compares_last_month',\n",
    "       'count_comments_last_month', 'count_group_membership_last_month']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def analyse_clusters(questions, users, analysis_user_variables, analysis_question_variables,\n",
    "                    cluster_question_dict, display_df=True):\n",
    " \n",
    "    results = []\n",
    "    index = []\n",
    "    for some_cluster in cluster_question_dict:\n",
    "    #     print('CLUSTER ',some_cluster)\n",
    "    #     print('-----------------------------------------------')\n",
    "        index.append(some_cluster)\n",
    "        results.append(user_analysis_cluster(cluster_question_dict[some_cluster], users,analysis_user_variables))\n",
    "        results[-1].update(question_analysis_cluster(cluster_question_dict[some_cluster],analysis_question_variables))\n",
    "\n",
    "\n",
    "    #print(results)\n",
    "    cluster_results = pd.DataFrame(results,index=index)\n",
    "    \n",
    "    if display_df=='True':\n",
    "        display(HTML(cluster_results.iloc[:10,:].to_html()))\n",
    "\n",
    "    print(cluster_results['num questions'].sum())\n",
    "    \n",
    "    return cluster_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def do_clustering(data, metric,dim_reduction_algorithm, dim_reduction_params, clustering_params, \n",
    "                  clustering_algorithm):\n",
    "    if dim_reduction_params['use_projection']==True:\n",
    "        data = data.T\n",
    "\n",
    "    if dim_reduction_algorithm in ['PCA','pca']:\n",
    "        if dim_reduction_params['use_projection']==True:\n",
    "            #do PCA\n",
    "            items_components = PCA_items_components(data,n_components = dim_reduction_params['n_components'],\n",
    "                                                    use_projection=True)\n",
    "            use_index = data.index\n",
    "        else:\n",
    "            items_components = PCA_items_components(data,n_components = dim_reduction_params['n_components'],\n",
    "                                                    use_projection=False, transpose=True)\n",
    "            use_index = data.columns\n",
    "\n",
    "    elif dim_reduction_algorithm in ['FA','fa','Factor Analysis']:\n",
    "        if dim_reduction_params['use_projection']==True:\n",
    "            #do FA\n",
    "            items_components = FA_items_components(data,n_components = dim_reduction_params['n_components'],\n",
    "                                                    use_projection=True)\n",
    "            use_index = data.index\n",
    "        else:\n",
    "            items_components = FA_items_components(data,n_components = dim_reduction_params['n_components'],\n",
    "                                                    use_projection=False, transpose=True)\n",
    "            use_index = data.columns\n",
    "    elif dim_reduction_algorithm in ['MDS','mds']:\n",
    "        items_components = MDS_items_components(data, n_components = dim_reduction_params['n_components'],\n",
    "                                                metric = dim_reduction_params['metric'],use_projection=True)\n",
    "        use_index = data.index\n",
    "    else:\n",
    "        items_components = None\n",
    "        \n",
    "    #do clustering\n",
    "    if clustering_algorithm==linkage:\n",
    "        if items_components is not None:\n",
    "            data_new = items_components\n",
    "            data_new.index = use_index\n",
    "            data_new.columns = ['Component '+str(i) for i in range(1,dim_reduction_params['n_components']+1)]\n",
    "            data = data_new\n",
    "        Z = pdist(data,clustering_params['metric'])\n",
    "        alg_with_params = linkage(Z, clustering_params['method'])\n",
    "        distance = squareform(Z)\n",
    "        hierarchy_clusters = fcluster(alg_with_params, clustering_params['t'], criterion=clustering_params['criterion'],depth=25)\n",
    "        \n",
    "        item_cluster_dict={item : hierarchy_clusters[i] for i, item in enumerate(data.index)}\n",
    "        cluster_items_dict = {i: [item for item, value in item_cluster_dict.items() if value==i] for i in item_cluster_dict.values()}\n",
    "    \n",
    "    else:\n",
    "            \n",
    "        alg_with_params = clustering_algorithm(**clustering_params)\n",
    "        distance = pairwise_distances(data, metric=metric)\n",
    "        distance_scaled = preprocessing.MinMaxScaler().fit_transform(distance)\n",
    "        \n",
    "        if clustering_algorithm==cluster.KMeans:\n",
    "            labels = alg_with_params.fit_predict(items_components)   \n",
    "\n",
    "        elif clustering_algorithm==cluster.DBSCAN:\n",
    "            similarity = 1 - distance_scaled\n",
    "            #print('shape is ',similarity.shape)\n",
    "            labels = alg_with_params.fit_predict(similarity)\n",
    "\n",
    "        elif clustering_algorithm==cluster.AgglomerativeClustering or\\\n",
    "        clustering_algorithm==cluster.SpectralClustering:\n",
    "            labels = alg_with_params.fit_predict(distance) \n",
    "\n",
    "        #columns are items, label is the one and only row\n",
    "        item_clusters = pd.DataFrame(labels,columns = ['label'], \\\n",
    "                                            index = use_index).T\n",
    "\n",
    "        #print('len of  clusters column', len(item_clusters.columns))\n",
    "        #display(HTML(item_clusters .iloc[:10,:10].to_html()))\n",
    "\n",
    "        # cluster: ['list of question ids']\n",
    "        cluster_items_dict = get_cluster_items_dict(item_clusters)\n",
    "\n",
    "        # question: cluster\n",
    "        item_cluster_dict = get_item_cluster_dict(item_clusters)\n",
    "\n",
    "    return distance, cluster_items_dict, item_cluster_dict, items_components, alg_with_params\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relevant_question_variables = ['id','liquid_vote_count','polarisation','count_comments','liquid_consensus'] \n",
    "\n",
    "\n",
    "relevant_user_variables = ['age','gender',\n",
    "        'region', 'count_comment_votes', 'count_friends',\n",
    "       'count_group_memberships', 'count_question_votes', 'count_questions',\n",
    "       'count_votes', 'count_choice_votes','count_following_tags', 'count_comments_user',\\\n",
    "        'count_followers','count_following_users', 'verification_score', 'verification_count',\n",
    "       'diff_perc_with_labour', 'diff_perc_with_conservative',\n",
    "       'diff_perc_with_green', 'diff_perc_with_plaidcymru',\n",
    "       'diff_perc_with_ukip', 'diff_perc_with_snp',\n",
    "       'diff_perc_with_liberaldemocrat', 'session_count',\n",
    "       'count_votes_last_3_months', 'count_choice_votes_last_3_months',\n",
    "       'count_compares_last_3_months', 'count_comments_last_3_months',\n",
    "       'count_group_membership_last_3_months', 'count_votes_last_month',\n",
    "       'count_choice_votes_last_month', 'count_compares_last_month',\n",
    "       'count_comments_last_month', 'count_group_membership_last_month']\n",
    "\n",
    "relevant_time_variables = ['modified_at','created_at']\n",
    "\n",
    "categorical_variables = ['region','gender','question_cluster', 'user_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sl_data(questions, users, question_clusters, user_clusters, user_variables,\n",
    "                time_variables, question_variables, categorical_variables, ignore_unanswered=True, tag_threshold=40):\n",
    "    questionvote_filtered = questionvote_table[(questionvote_table\\\n",
    "                                            ['object_id'].isin(questions))\\\n",
    "                                           & questionvote_table['user_id'].isin(users)]\n",
    "\n",
    "    sl_data = pd.merge(questionvote_filtered, users_info_table, left_on='user_id',right_on='id')\n",
    "    \n",
    "    questions_with_tags_filtered = questions_with_tags[questions_with_tags.id.isin(questions)]\n",
    "    \n",
    "    questions_with_tags_filtered = questions_with_tags_filtered[question_variables+['tags']]\n",
    "    \n",
    "    sl_data = pd.merge(sl_data, questions_with_tags_filtered,left_on='object_id',right_on='id',\\\n",
    "                          suffixes=('_user','_question'))\n",
    "    \n",
    "    for time_variable in time_variables:\n",
    "        sl_data[time_variable] = sl_data[time_variable].astype(int)\n",
    "\n",
    "    sl_data = shuffle(sl_data)\n",
    "    \n",
    "    if ignore_unanswered:\n",
    "        sl_data.dropna(subset=['value'],inplace=True) \n",
    "        \n",
    "    if type(question_clusters)==dict:\n",
    "        sl_data['question_cluster'] = sl_data['object_id'].map(question_clusters)\n",
    "    if type(user_clusters)==dict:\n",
    "        sl_data['user_cluster'] = sl_data['user_id'].map(user_clusters)\n",
    "    print('SL DATA COLUMNS',sl_data.columns) \n",
    "    new_question_variables = ['liquid_vote_count','polarisation','count_comments_question','liquid_consensus'] \n",
    "    X = sl_data[[column for column in relevant_user_variables +  relevant_time_variables + new_question_variables +['object_id'] if column\\\n",
    "                             not in categorical_variables]] #forget about categorical for now \n",
    "\n",
    "#------------------------------------TAGS FEATURE ENGINEERING - CURRENTLY OMITTED------------------#\n",
    "#     questions_df = questions_with_tags[questions_with_tags.id.isin(questions)]\n",
    "#     #list of tags for every question\n",
    "#     all_tags = re.split(', |\\|',questions_df.fillna(value='None').tags.str.cat(sep='|'))\n",
    "\n",
    "#     #count the tags\n",
    "#     cnt = Counter(all_tags)\n",
    "#     most_common_tags = [i[0] for i in list(cnt.most_common(tag_threshold))]\n",
    "\n",
    "#     q_with_list_tags = questions_df['tags'].fillna(value='None').\\\n",
    "#     apply(lambda x: x.split(', '))\n",
    "\n",
    "#     mlb = preprocessing.MultiLabelBinarizer()\n",
    "#     tags = mlb.fit_transform(q_with_list_tags)   \n",
    "#     #print(tags)\n",
    "#     tags = pd.DataFrame(tags, columns=mlb.classes_,index=questions_df.id)[most_common_tags]\n",
    "\n",
    "    #X = pd.merge(X,tags,right_index=True,left_on='object_id')\n",
    "\n",
    "    X.rename(columns={'count_comments':'count_comments_question'},inplace=True)\n",
    "    X[['modified_at','created_at']]= preprocessing.MinMaxScaler().fit_transform(X[['modified_at','created_at']])\n",
    "    for category in categorical_variables:\n",
    "        X = pd.concat([X, pd.get_dummies(sl_data[category],prefix=category,dummy_na=True)],axis=1)\n",
    "        \n",
    "    #print('len of train data', len(X), 'columns: ', X.columns.tolist()) \n",
    "    X.fillna(value=0, inplace=True)\n",
    "    X.drop(['object_id'],axis=1,inplace=True)\n",
    "    #print(X.columns[20:])\n",
    "    Y = sl_data['value']\n",
    "    return X,Y\n",
    "\n",
    "def gridsearch_cv(X,Y,estimator, cv, params, scoring, verbose, error_score='raise', n_jobs=-1):\n",
    "    gsv = GridSearchCV(estimator=estimator, param_grid=params, cv=cv,\\\n",
    "                             scoring=scoring, verbose=verbose, error_score=error_score,n_jobs=n_jobs)\n",
    "    gsv.fit(X,Y)\n",
    "    return gsv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4015 users\n",
      "268 questions\n",
      "the scaling function is  StandardScaler(copy=True, with_mean=True, with_std=True)\n",
      "the len of final comb is 12\n",
      "the input sizes are:  3 x 2 x 2\n",
      "fitting 1 out of 12\n",
      "{'n_init': 10, 'n_clusters': 10, 'n_jobs': -1, 'affinity': 'sigmoid'}\n",
      "Explained variance is  0.657873960269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/cluster/spectral.py:439: UserWarning: The spectral clustering API has changed. ``fit``now constructs an affinity matrix from data. To use a custom affinity matrix, set ``affinity=precomputed``.\n",
      "  warnings.warn(\"The spectral clustering API has changed. ``fit``\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance is  0.641746839654\n",
      "fitting 2 out of 12\n",
      "{'n_init': 10, 'n_clusters': 10, 'n_jobs': -1, 'affinity': 'sigmoid'}\n",
      "Explained variance is  0.657873960269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/cluster/spectral.py:439: UserWarning: The spectral clustering API has changed. ``fit``now constructs an affinity matrix from data. To use a custom affinity matrix, set ``affinity=precomputed``.\n",
      "  warnings.warn(\"The spectral clustering API has changed. ``fit``\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance is  0.641746839655\n",
      "fitting 3 out of 12\n",
      "{'n_init': 10, 'n_clusters': 20, 'n_jobs': -1, 'affinity': 'sigmoid'}\n",
      "Explained variance is  0.657873960269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/cluster/spectral.py:439: UserWarning: The spectral clustering API has changed. ``fit``now constructs an affinity matrix from data. To use a custom affinity matrix, set ``affinity=precomputed``.\n",
      "  warnings.warn(\"The spectral clustering API has changed. ``fit``\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance is  0.641746839652\n",
      "fitting 4 out of 12\n",
      "{'n_init': 10, 'n_clusters': 20, 'n_jobs': -1, 'affinity': 'sigmoid'}\n",
      "Explained variance is  0.657873960269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/cluster/spectral.py:439: UserWarning: The spectral clustering API has changed. ``fit``now constructs an affinity matrix from data. To use a custom affinity matrix, set ``affinity=precomputed``.\n",
      "  warnings.warn(\"The spectral clustering API has changed. ``fit``\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance is  0.641746839656\n",
      "fitting 5 out of 12\n",
      "{'n_init': 10, 'n_clusters': 10, 'n_jobs': -1, 'affinity': 'sigmoid'}\n",
      "Explained variance is  0.753920256939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/cluster/spectral.py:439: UserWarning: The spectral clustering API has changed. ``fit``now constructs an affinity matrix from data. To use a custom affinity matrix, set ``affinity=precomputed``.\n",
      "  warnings.warn(\"The spectral clustering API has changed. ``fit``\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance is  0.734065751476\n",
      "fitting 6 out of 12\n",
      "{'n_init': 10, 'n_clusters': 10, 'n_jobs': -1, 'affinity': 'sigmoid'}\n",
      "Explained variance is  0.753939087835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/cluster/spectral.py:439: UserWarning: The spectral clustering API has changed. ``fit``now constructs an affinity matrix from data. To use a custom affinity matrix, set ``affinity=precomputed``.\n",
      "  warnings.warn(\"The spectral clustering API has changed. ``fit``\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance is  0.734073947913\n",
      "fitting 7 out of 12\n",
      "{'n_init': 10, 'n_clusters': 20, 'n_jobs': -1, 'affinity': 'sigmoid'}\n",
      "Explained variance is  0.753899589696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/cluster/spectral.py:439: UserWarning: The spectral clustering API has changed. ``fit``now constructs an affinity matrix from data. To use a custom affinity matrix, set ``affinity=precomputed``.\n",
      "  warnings.warn(\"The spectral clustering API has changed. ``fit``\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance is  0.734038769878\n",
      "fitting 8 out of 12\n",
      "{'n_init': 10, 'n_clusters': 20, 'n_jobs': -1, 'affinity': 'sigmoid'}\n",
      "Explained variance is  0.753866619619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/cluster/spectral.py:439: UserWarning: The spectral clustering API has changed. ``fit``now constructs an affinity matrix from data. To use a custom affinity matrix, set ``affinity=precomputed``.\n",
      "  warnings.warn(\"The spectral clustering API has changed. ``fit``\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance is  0.734021307846\n",
      "fitting 9 out of 12\n",
      "{'n_init': 10, 'n_clusters': 10, 'n_jobs': -1, 'affinity': 'sigmoid'}\n",
      "Explained variance is  0.82211668862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/cluster/spectral.py:439: UserWarning: The spectral clustering API has changed. ``fit``now constructs an affinity matrix from data. To use a custom affinity matrix, set ``affinity=precomputed``.\n",
      "  warnings.warn(\"The spectral clustering API has changed. ``fit``\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance is  0.807529353757\n",
      "fitting 10 out of 12\n",
      "{'n_init': 10, 'n_clusters': 10, 'n_jobs': -1, 'affinity': 'sigmoid'}\n",
      "Explained variance is  0.82213436936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/cluster/spectral.py:439: UserWarning: The spectral clustering API has changed. ``fit``now constructs an affinity matrix from data. To use a custom affinity matrix, set ``affinity=precomputed``.\n",
      "  warnings.warn(\"The spectral clustering API has changed. ``fit``\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance is  0.807766044679\n",
      "fitting 11 out of 12\n",
      "{'n_init': 10, 'n_clusters': 20, 'n_jobs': -1, 'affinity': 'sigmoid'}\n",
      "Explained variance is  0.822440229742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/cluster/spectral.py:439: UserWarning: The spectral clustering API has changed. ``fit``now constructs an affinity matrix from data. To use a custom affinity matrix, set ``affinity=precomputed``.\n",
      "  warnings.warn(\"The spectral clustering API has changed. ``fit``\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance is  0.807577966128\n",
      "fitting 12 out of 12\n",
      "{'n_init': 10, 'n_clusters': 20, 'n_jobs': -1, 'affinity': 'sigmoid'}\n",
      "Explained variance is  0.822540644822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/sklearn/cluster/spectral.py:439: UserWarning: The spectral clustering API has changed. ``fit``now constructs an affinity matrix from data. To use a custom affinity matrix, set ``affinity=precomputed``.\n",
      "  warnings.warn(\"The spectral clustering API has changed. ``fit``\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance is  0.807680520042\n",
      "took 1381.343s\n"
     ]
    }
   ],
   "source": [
    "kmeans_params_u_grid = {'init':['k-means++'], 'n_clusters':[20,40,50],'n_init':[10]} \n",
    "kmeans_params_q_grid = {'init':['k-means++'], 'n_clusters':[10,15,20],'n_init':[5]}#'n_jobs':[-1]} \n",
    "hierarchy_params_q = {'metric':['euclidean','correlation','cityblock'],'method':['ward'],'criterion':['maxclust'],'t':[5,10,20]}\n",
    "hierarchy_params_u = {'metric':['euclidean','correlation','cityblock'],'method':['ward'],'criterion':['maxclust'],'t':[5,20,40]}\n",
    "spectral_poly_u = {'n_init':[10],'n_clusters':[10,20],'affinity':['sigmoid'],'n_jobs':[-1]}\n",
    "spectral_poly_q = {'n_init':[5],'n_clusters':[5,10],'affinity':['sigmoid'],'n_jobs':[-1]}\n",
    "\n",
    "dim_reduction_params_grid = {'n_components':[5,20,50],'use_projection':[True]}\n",
    "mds_params_grid = {'n_components':[50],'metric':[True, False],'use_projection':[True]}\n",
    "data = create_vote_data(None, british_users, question_threshold=500,\n",
    "                            user_threshold=20, fillna_value=0, rescale_data=False)\n",
    "data = scale_data(data)\n",
    "\n",
    "users = data.index.tolist() #users with threshold\n",
    "questions = data.columns.tolist() #questions with threshold\n",
    "\n",
    "\n",
    "metric = 'euclidean'\n",
    "def my_gridsearch(model,clustering,dim_red,clust_params_u,clust_params_q,start=None, threshold=None):\n",
    "    start = timeit.default_timer()\n",
    "    \n",
    "    dim_red_combinations = [dict(zip(dim_red, values)) \n",
    "                for values in itertools.product(*dim_red.values())]\n",
    "    clust_u_combinations = [dict(zip(clust_params_u, values)) \n",
    "                for values in itertools.product(*clust_params_u.values())]\n",
    "    clust_q_combinations = [dict(zip(clust_params_q, values)) \n",
    "                for values in itertools.product(*clust_params_q.values())]\n",
    "    final_comb = list(itertools.product(dim_red_combinations, clust_u_combinations,clust_q_combinations))\n",
    "    print('the len of final comb is', len(final_comb))\n",
    "    print('the input sizes are: ',len(dim_red_combinations),'x',len(clust_u_combinations),\n",
    "          'x',len(clust_q_combinations))\n",
    "    scores = {}\n",
    "    if threshold!=None:\n",
    "        final_comb = final_comb[:threshold]\n",
    "        \n",
    "    for i, params_set in enumerate(final_comb):\n",
    "        print('fitting '+str(i+1)+' out of '+str(len(final_comb)))\n",
    "        dim_params = params_set[0]\n",
    "        #print(dim_params)\n",
    "        user_params = params_set[1]\n",
    "        print(user_params)\n",
    "        question_params = params_set[2]\n",
    "        s = 'dim_params: '+ str(['{} {}'.format(k,v) for k,v in dim_params.items()])\\\n",
    "        + ' user_params: ' + str(['{} {}'.format(k,v) for k,v in user_params.items()])\\\n",
    "        + ' question_params: ' + str(['{} {}'.format(k,v) for k,v in question_params.items()]) \n",
    "        for iteration in range(1):\n",
    "            question_distance, cluster_question_dict, question_cluster_dict, questions_components, alg_q  = do_clustering(data.T, metric,\n",
    "                                                                                                              'PCA',dim_params,\n",
    "                                                                                                              question_params,clustering)\n",
    "\n",
    "            user_distance, cluster_user_dict, user_cluster_dict,users_components, alg_u = do_clustering(data, metric,\n",
    "                                                                                            'PCA',dim_params,\n",
    "                                                                                                user_params,clustering)\n",
    "\n",
    "            X,Y = get_sl_data(questions, users, question_cluster_dict, user_cluster_dict, relevant_user_variables,\n",
    "                  relevant_time_variables, relevant_question_variables, categorical_variables)\n",
    "            res = cross_val_score(model,X,Y,cv=3)\n",
    "            if type(scores.get(s))!=type(None):\n",
    "                if np.mean(res) > np.mean(scores.get(s)):\n",
    "                    scores[s] = res\n",
    "            else:\n",
    "                scores[s] = res\n",
    "    end = timeit.default_timer() - start\n",
    "    print('took {:.3f}s'.format(end))\n",
    "    return scores\n",
    "\n",
    "sgd = SGDClassifier(max_iter=300,loss='hinge',verbose=0,n_jobs=-1)\n",
    "rf = RandomForestClassifier(n_estimators=100,max_features=None,oob_score=False,verbose=0,n_jobs=-1)\n",
    "\n",
    "a = my_gridsearch(sgd,cluster.SpectralClustering,dim_reduction_params_grid,spectral_poly_u,spectral_poly_q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"dim_params: ['n_components 20', 'use_projection True'] user_params: ['n_init 10', 'n_clusters 10', 'n_jobs -1', 'affinity sigmoid'] question_params: ['n_init 5', 'n_clusters 5', 'n_jobs -1', 'affinity sigmoid']\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_res={}\n",
    "for i in a:\n",
    "    mean_res[i] = np.mean(sorted(a[i])[-2:])\n",
    "max(mean_res,key=mean_res.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Iter       Train Loss   Remaining Time \n",
      "         1      290171.8982           52.47m\n",
      "         2      276874.9095           51.67m\n",
      "         3      266191.0163           50.33m\n",
      "         4      257346.2013           48.46m\n",
      "         5      249832.9304           46.69m\n",
      "         6      243339.4049           45.67m\n",
      "         7      237819.2908           44.50m\n",
      "         8      232825.4191           43.28m\n",
      "         9      228469.8929           41.95m\n",
      "        10      224679.8726           40.70m\n",
      "        11      221317.3095           39.44m\n",
      "        12      218285.1144           38.25m\n",
      "        13      215493.2852           37.02m\n",
      "        14      213021.0738           35.81m\n",
      "        15      210726.2595           34.64m\n",
      "        16      208615.2553           33.47m\n",
      "        17      206582.2537           32.29m\n",
      "        18      204700.7168           31.18m\n",
      "        19      202906.3473           30.09m\n",
      "        20      201397.4551           28.90m\n",
      "        21      199706.1918           27.80m\n",
      "        22      198155.1661           26.70m\n",
      "        23      196818.4264           25.54m\n",
      "        24      195380.9327           31.84m\n",
      "        25      193993.1799           30.26m\n",
      "        26      192839.5441           28.69m\n",
      "        27      191695.9599           27.29m\n",
      "        28      190598.7564           25.87m\n",
      "        29      189519.5482           24.45m\n",
      "        30      188385.3578           23.13m\n",
      "        31      187359.1088           21.81m\n",
      "        32      186553.1964           20.47m\n",
      "        33      185335.8288           19.20m\n",
      "        34      184482.5407           17.94m\n",
      "        35      183696.0279           16.67m\n",
      "        36      182669.6954           15.44m\n",
      "        37      181917.2748           14.25m\n",
      "        38      181169.6592           13.03m\n",
      "        39      180364.8067           11.83m\n",
      "        40      179602.5265           10.68m\n",
      "        41      178876.1567            9.59m\n",
      "        42      178146.1946            8.45m\n",
      "        43      177414.9212            7.36m\n",
      "        44      176815.1018            6.26m\n",
      "        45      175950.7971            5.20m\n",
      "        46      175367.5315            4.13m\n",
      "        47      174740.4191            3.08m\n",
      "        48      174194.6402            2.04m\n",
      "        49      173408.0859            1.01m\n",
      "        50      172715.8325            0.00s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=10,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=50,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=2,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc = GradientBoostingClassifier(n_estimators=50,verbose=2, max_depth=10,min_samples_split=2, min_samples_leaf=1)\n",
    "\n",
    "gbc.fit(X[:int(2/3*(len(X)))],Y[:int(2/3*(len(X)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_imp=pd.DataFrame(gbc.feature_importances_*100, index=X.columns, columns=['importance']).sort_values(by='importance',ascending=False)\n",
    "\n",
    "feat_imp.iloc[:10,:].plot(kind='bar')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
